{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q transformers peft datasets bitsandbytes accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Torch: 2.8.0+cu126\n",
      "üíª GPU: Tesla T4\n",
      "üêç Python: 3.12.12\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"‚úÖ Torch:\", torch.__version__)\n",
    "print(\"üíª GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "print(\"üêç Python:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üß† QLoRA Fine-Tuning Script ‚Äì Llama-3-8B-Instruct\n",
    "# ------------------------------------------------------------\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "OUTPUT_DIR = \"./lora-weights\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"üîπ Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LoRA setup\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Small public dataset (you can replace later)\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1%]\")\n",
    "\n",
    "\n",
    "def format(example):\n",
    "    return {\"text\": f\"<|user|>: {example['instruction']}\\n<|assistant|>: {example['output']}\"}\n",
    "dataset = dataset.map(format)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    tokens = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.03,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"üíæ Saving LoRA adapter...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"‚úÖ LoRA adapter saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029940733b5b4bc9904df3d298832935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1368e640f5df4ba6a5b37d08fc8e46c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7949cf94e4eb45548e1790dbd382c244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2348f0f966614ab290d42916c312a1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeef649ce39f490f8fd4f2ff5ce3e155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598dcd29f5084e108d8990d8903f32f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ce4d5562f149149672c4df1d1731ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8896406e6f444985b60ab0ea85b708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa4e9efa54043efbd88126e3ac95484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading LoRA adapter...\n",
      "üîπ Merging LoRA weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving merged model...\n",
      "‚úÖ Merged model saved to ./merged-llama3\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "LORA_PATH = \"./lora-weights\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"üîπ Loading base model...\")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_folder=\"./offload\",  # ‚úÖ add this line\n",
    ")\n",
    "\n",
    "print(\"üîπ Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base,\n",
    "    LORA_PATH,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"   # ‚úÖ add here too\n",
    ")\n",
    "\n",
    "print(\"üîπ Merging LoRA weights...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(\"üíæ Saving merged model...\")\n",
    "merged_model.save_pretrained(\"./merged-llama3\")\n",
    "print(\"‚úÖ Merged model saved to ./merged-llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd62c28f9ff540718aca4f09cbcb6cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389d061e96054faa83b164e439c15366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3533a0423a3a41e5a5dff5b0c2168d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer files copied successfully into: merged-llama3\n",
      "chat_template.jinja\t\t  model.safetensors.index.json\n",
      "config.json\t\t\t  special_tokens_map.json\n",
      "generation_config.json\t\t  tokenizer_config.json\n",
      "model-00001-of-00002.safetensors  tokenizer.json\n",
      "model-00002-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Copy tokenizer from base model to merged folder\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "merged_path = \"merged-llama3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(\"‚úÖ Tokenizer files copied successfully into:\", merged_path)\n",
    "!ls {merged_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading tokenizer & model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b900f66e6744a793b7dcd8dc795194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully with 4-bit quantization!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4229318752.py:113: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Launching Gradio App ...\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://9cb619bea95ad61de1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9cb619bea95ad61de1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ü¶ô Llama-3-8B-Instruct (Merged + Polite & Descriptive Output)\n",
    "# ------------------------------------------------------------\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import re\n",
    "\n",
    "MODEL_NAME = \"merged-llama3\"\n",
    "\n",
    "# ‚úÖ System prompt tuned for polite, natural, short-descriptive answers\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a friendly, polite, and knowledgeable AI assistant. \"\n",
    "    \"Always start with a short greeting like 'Hello!' or 'Good day!'. \"\n",
    "    \"Provide clear, concise, and slightly descriptive answers (2‚Äì4 lines). \"\n",
    "    \"Avoid showing internal roles, system tags, or code. \"\n",
    "    \"Focus only on the user's question, keep the response natural and human-like.\"\n",
    ")\n",
    "history_token_total = 0\n",
    "\n",
    "print(\"üîπ Loading tokenizer & model ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- 4-bit quantization config (for Tesla T4) -------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully with 4-bit quantization!\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß† Clean & Chat Function\n",
    "# ------------------------------------------------------------\n",
    "def clean_output(text: str) -> str:\n",
    "    \"\"\"Remove unwanted role labels and artifacts.\"\"\"\n",
    "    text = re.sub(r\"(<\\|.*?\\|>|system|user|assistant|###)\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    global history_token_total\n",
    "\n",
    "    if message.strip().lower() in {\"exit\", \"quit\"}:\n",
    "        history.append((message, \"üëã Goodbye! Have a great day!\"))\n",
    "        return history, \"\", 0, 0, history_token_total, 0\n",
    "\n",
    "    # --- Build chat context -------------------\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    for u, a in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": u})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # --- Tokenize -----------------------------\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # --- Generate polite + descriptive answer ---\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=180,\n",
    "        temperature=0.5,       # slightly creative, still controlled\n",
    "        top_p=0.85,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # --- Decode + clean ------------------------\n",
    "    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    reply = clean_output(reply)\n",
    "\n",
    "    # Extract assistant portion if present\n",
    "    if message in reply:\n",
    "        reply = reply.split(message)[-1].strip()\n",
    "    if \"Assistant:\" in reply:\n",
    "        reply = reply.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "    # --- Token tracking ------------------------\n",
    "    output_ids = tokenizer(reply, return_tensors=\"pt\").input_ids\n",
    "    output_tokens = output_ids.shape[1]\n",
    "    history_token_total += input_tokens + output_tokens\n",
    "    history.append((message, reply))\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "\n",
    "    return history, \"\", input_tokens, output_tokens, history_token_total, total_tokens\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üí¨ Gradio Interface\n",
    "# ------------------------------------------------------------\n",
    "with gr.Blocks(title=\"ü¶ô Llama-3 QLoRA Chatbot\") as demo:\n",
    "    gr.Markdown(\"## ü¶ô Llama-3-8B QLoRA Chatbot (Polite & Descriptive Answers)\")\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "        txt = gr.Textbox(\n",
    "            label=\"üí¨ Type your question and press Enter‚Ä¶\",\n",
    "            placeholder=\"Ask about AI, tech, or general knowledge...\"\n",
    "        )\n",
    "    with gr.Row():\n",
    "        in_tok = gr.Number(label=\"Input Tokens\", value=0, interactive=False)\n",
    "        out_tok = gr.Number(label=\"Output Tokens\", value=0, interactive=False)\n",
    "        hist_tok = gr.Number(label=\"History Tokens\", value=0, interactive=False)\n",
    "        total_tok = gr.Number(label=\"Total Tokens\", value=0, interactive=False)\n",
    "\n",
    "    txt.submit(\n",
    "        chat_fn,\n",
    "        [txt, chatbot],\n",
    "        [chatbot, txt, in_tok, out_tok, hist_tok, total_tok]\n",
    "    )\n",
    "\n",
    "print(\"üöÄ Launching Gradio App ...\")\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
