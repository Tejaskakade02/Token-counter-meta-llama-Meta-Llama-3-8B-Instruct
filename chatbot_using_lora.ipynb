{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qar57DmIC--d",
        "outputId": "7c6ab865-7a87-444b-e0a4-3483a7aacd81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate triton.ops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMEQSBpWH9x3",
        "outputId": "b06d9fd8-3aef-4fb8-e869-3bfbd4f864ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement triton.ops (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for triton.ops\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install accelerate transformers peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMcGz3ElJfwf",
        "outputId": "08ebe9ae-4006-4c8f-e784-875b7c79fca1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y bitsandbytes triton\n",
        "!pip install bitsandbytes triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqpiHF9gSNaE",
        "outputId": "6a6c2544-c2f6-41fe-b3d4-574e12ddbf9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "import triton\n",
        "import torch\n",
        "print(\"bitsandbytes:\", bnb.__version__)\n",
        "print(\"triton:\", triton.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O4s_3OrJwft",
        "outputId": "215d80a3-6c89-4717-889e-f328a37b060a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bitsandbytes: 0.48.2\n",
            "triton: 3.4.0\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"your_hf_permission_token\")  # üëâ paste your Hugging Face access token when prompted\n"
      ],
      "metadata": {
        "id": "_RzbfEBcDyZ6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# ü¶ô Llama-3-8B-Instruct ‚Äì QLoRA Chatbot (Clean, Full Display)\n",
        "# ------------------------------------------------------------\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "import re\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Model Setup\n",
        "# ------------------------------------------------------------\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "print(\"üîπ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ‚úÖ QLoRA (4-bit quantization)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "print(\"üîπ Loading quantized model (QLoRA 4-bit)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(\"\\nü§ñ Chatbot Loaded Successfully!\\n\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Token Filters\n",
        "# ------------------------------------------------------------\n",
        "SYSTEM_PROMPT = \"You are a helpful and knowledgeable assistant.\"\n",
        "history_token_total = 0\n",
        "\n",
        "UNNECESSARY = {\n",
        "    '', '\\n', '<', '|', '>', 'system', 'user', 'assistant',\n",
        "    '_', 'start', 'end',\n",
        "    '<|begin_of_text|>', '<|eot_id|>', '<|end_of_text|>',\n",
        "    '<|end_header_id|>', '<|start_header_id|>', ' '\n",
        "}\n",
        "SPECIAL_PATTERN = re.compile(r\"(<\\|.*?\\|>|ƒä|ƒ†| |:|>|<|\\\\n)\")\n",
        "\n",
        "def clean_tokens(tokens):\n",
        "    \"\"\"Remove special or unwanted tokens.\"\"\"\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "        t = SPECIAL_PATTERN.sub(\"\", t).strip()\n",
        "        if t and t not in UNNECESSARY:\n",
        "            cleaned.append(t)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Chat Function\n",
        "# ------------------------------------------------------------\n",
        "def chat_fn(message, history):\n",
        "    global history_token_total\n",
        "\n",
        "    if message.strip().lower() in {\"exit\", \"quit\"}:\n",
        "        history.append((message, \"üëã Goodbye! Chat session ended.\"))\n",
        "        return history, 0, 0, history_token_total, 0, \"\", \"\"\n",
        "\n",
        "    # üß† Build conversation context\n",
        "    conversation = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "        + [{\"role\": \"user\", \"content\": u, \"role\": \"assistant\", \"content\": a} for u, a in history]\n",
        "        + [{\"role\": \"user\", \"content\": message}],\n",
        "        tokenize=False\n",
        "    )\n",
        "\n",
        "    # 1Ô∏è‚É£ Encode + count input tokens\n",
        "    input_ids = tokenizer(conversation, return_tensors=\"pt\").input_ids\n",
        "    input_tokens = input_ids.shape[1]\n",
        "    input_token_text = clean_tokens(tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
        "\n",
        "    # 2Ô∏è‚É£ Generate response (ensure full, readable output)\n",
        "    outputs = pipe(\n",
        "        conversation,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    generated_text = outputs[0][\"generated_text\"]\n",
        "\n",
        "    # Extract clean assistant response\n",
        "    if \"Assistant:\" in generated_text:\n",
        "        reply = generated_text.split(\"Assistant:\")[-1].strip()\n",
        "    elif \"assistant\" in generated_text.lower():\n",
        "        reply = generated_text.split(\"assistant\")[-1].strip(\": \").strip()\n",
        "    else:\n",
        "        reply = generated_text.strip()\n",
        "\n",
        "    # Make sure the reply ends cleanly (avoid incomplete cutoffs)\n",
        "    reply = re.sub(r\"(\\n)+$\", \"\", reply).strip()\n",
        "\n",
        "    # 3Ô∏è‚É£ Count output tokens\n",
        "    output_ids = tokenizer(reply, return_tensors=\"pt\").input_ids\n",
        "    output_tokens = output_ids.shape[1]\n",
        "    output_token_text = clean_tokens(tokenizer.convert_ids_to_tokens(output_ids[0]))\n",
        "\n",
        "    # 4Ô∏è‚É£ Update totals\n",
        "    history_token_total += input_tokens + output_tokens\n",
        "    total_tokens = input_tokens + output_tokens\n",
        "\n",
        "    # üó®Ô∏è Show message + reply in UI\n",
        "    history.append((message, reply))\n",
        "\n",
        "    return (\n",
        "        history,\n",
        "        input_tokens,\n",
        "        output_tokens,\n",
        "        history_token_total,\n",
        "        total_tokens,\n",
        "        str(input_token_text),\n",
        "        str(output_token_text),\n",
        "    )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Gradio UI\n",
        "# ------------------------------------------------------------\n",
        "with gr.Blocks(title=\"ü¶ô Llama-3 QLoRA Chatbot (Full)\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # ü¶ô Llama-3-8B QLoRA Chatbot\n",
        "        **Now with proper responses and visible messages!**\n",
        "        Tracks: Input Tokens ‚Ä¢ Output Tokens ‚Ä¢ History Tokens ‚Ä¢ Total Tokens\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Chat History\",\n",
        "                height=500,\n",
        "                bubble_full_width=False,\n",
        "                show_label=True\n",
        "            )\n",
        "            txt = gr.Textbox(\n",
        "                placeholder=\"Type your question and press Enter...\",\n",
        "                label=\"üí¨ Message\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            in_tok = gr.Number(label=\"üß© Input Tokens\", value=0, interactive=False)\n",
        "            out_tok = gr.Number(label=\"üß† Output Tokens\", value=0, interactive=False)\n",
        "            hist_tok = gr.Number(label=\"üìú History Tokens\", value=0, interactive=False)\n",
        "            total_tok = gr.Number(label=\"üî¢ Total Tokens\", value=0, interactive=False)\n",
        "\n",
        "            gr.Markdown(\"### üîç Clean Token Display\")\n",
        "            in_tok_text = gr.Textbox(label=\"üì• Input Tokens (cleaned)\", lines=4, interactive=False)\n",
        "            out_tok_text = gr.Textbox(label=\"üì§ Output Tokens (cleaned)\", lines=4, interactive=False)\n",
        "\n",
        "    def clear_fn():\n",
        "        global history_token_total\n",
        "        history_token_total = 0\n",
        "        return [], \"\", 0, 0, 0, 0, \"\", \"\"\n",
        "\n",
        "    clear_btn = gr.Button(\"üßπ Clear Chat\")\n",
        "\n",
        "    txt.submit(\n",
        "        chat_fn,\n",
        "        [txt, chatbot],\n",
        "        [chatbot, in_tok, out_tok, hist_tok, total_tok, in_tok_text, out_tok_text],\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        clear_fn,\n",
        "        None,\n",
        "        [chatbot, txt, in_tok, out_tok, hist_tok, total_tok, in_tok_text, out_tok_text],\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. Launch App\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=False)\n"
      ],
      "metadata": {
        "id": "Wa8XTYMHERwn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}