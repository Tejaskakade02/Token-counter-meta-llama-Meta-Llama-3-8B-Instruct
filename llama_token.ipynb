{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.120.1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
      "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) y\n",
      "Token is valid (permission: read).\n",
      "The token `ollama3:8b` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `ollama3:8b`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fd7244caf946bebc3c8d986686c124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa91edd7153421892cf06b8c7eefa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0163c0415da74f67af21eb25168f44ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67deef52e9064aef9eb0d75a3ae4fc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018403b62db54e568c51e3d4b938edb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186f9d1d36ab41fba32b55b4f89519ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4952534badc84f528bd06ef0368b1db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5726ccd1bc7c44e58026dd3e63a7f018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b7fdeec0544efea8fd52375eecb514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a438aaa5252c42c98300d071ecc2b7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7860da2e179b4273b9213c7f6f02a090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85bef5744944359b52ece74d9d817bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/tmp/ipython-input-3161537769.py:149: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(async (port, path, width, height, cache, element) => {\n",
       "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
       "                            return;\n",
       "                        }\n",
       "                        element.appendChild(document.createTextNode(''));\n",
       "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
       "\n",
       "                        const external_link = document.createElement('div');\n",
       "                        external_link.innerHTML = `\n",
       "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
       "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
       "                                    https://localhost:${port}${path}\n",
       "                                </a>\n",
       "                            </div>\n",
       "                        `;\n",
       "                        element.appendChild(external_link);\n",
       "\n",
       "                        const iframe = document.createElement('iframe');\n",
       "                        iframe.src = new URL(path, url).toString();\n",
       "                        iframe.height = height;\n",
       "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
       "                        iframe.width = width;\n",
       "                        iframe.style.border = 0;\n",
       "                        element.appendChild(iframe);\n",
       "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Load model\n",
    "# -------------------------------------------------\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"   # gated – run `huggingface-cli login`\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Globals\n",
    "# -------------------------------------------------\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "conversation = []                     # list of {\"role\":..., \"content\":...}\n",
    "all_input_counts   = []               # content-only token count per turn\n",
    "all_output_counts  = []               # raw output token count\n",
    "all_input_strings  = []               # display list (no Ġ)\n",
    "all_output_strings = []               # display list (no Ġ)\n",
    "all_input_raw      = []               # raw token strings (with Ġ) – for decoding\n",
    "all_output_raw     = []               # raw token strings (with Ġ) – for decoding\n",
    "\n",
    "UNNECESSARY = {\n",
    "    '', '\\n', '<', '|', '>', 'system', 'user', 'ass', 'istant',\n",
    "    'im', '_', 'start', 'end',\n",
    "    '<|begin_of_text|>', '<|eot_id|>', '<|end_of_text|>',\n",
    "    '<|end_header_id|>', '<|start_header_id|>'\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Chat function\n",
    "# -------------------------------------------------\n",
    "def chat(user_input: str, history: list):\n",
    "    global conversation, all_input_counts, all_output_counts\n",
    "    global all_input_strings, all_output_strings, all_input_raw, all_output_raw\n",
    "\n",
    "    # ---- 1. add user message ----\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # ---- 2. content-only prompt (system + history + current) ----\n",
    "    content = SYSTEM_PROMPT + \" \"\n",
    "    for m in conversation[:-1]:\n",
    "        content += m[\"content\"] + \" \"\n",
    "    content += user_input\n",
    "\n",
    "    # ---- 3. tokenise content-only (no chat template) ----\n",
    "    ids = tokenizer(content, return_tensors=\"pt\").to(model.device).input_ids[0]\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(ids)                 # keep Ġ\n",
    "    clean_tokens = [t.replace(\"Ġ\", \"\") for t in raw_tokens\n",
    "                    if t not in UNNECESSARY and t.strip() != \"\\n\"]\n",
    "\n",
    "    # ---- 4. store input info ----\n",
    "    all_input_counts.append(len(clean_tokens))\n",
    "    all_input_strings.append(clean_tokens)\n",
    "    all_input_raw.append(raw_tokens)\n",
    "\n",
    "    # ---- 5. generate with full chat template (needed for the model) ----\n",
    "    msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + conversation\n",
    "    full_prompt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    full_ids = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device).input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            full_ids,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # ---- 6. extract only the newly generated tokens ----\n",
    "    new_ids = out_ids[0][full_ids.shape[1]:]\n",
    "    reply = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    out_raw = tokenizer.convert_ids_to_tokens(new_ids)\n",
    "    out_clean = [t.replace(\"Ġ\", \"\") for t in out_raw\n",
    "                 if t not in UNNECESSARY and t.strip() != \"\\n\"]\n",
    "\n",
    "    # ---- 7. store output info ----\n",
    "    all_output_counts.append(len(new_ids))\n",
    "    all_output_strings.append(out_clean)\n",
    "    all_output_raw.append(out_raw)\n",
    "\n",
    "    # ---- 8. update conversation history (keep only last 20 turns) ----\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    if len(conversation) > 20:\n",
    "        conversation = conversation[-20:]\n",
    "\n",
    "    # ---- 9. Gradio history ----\n",
    "    history = history or []\n",
    "    history.append((user_input, reply))\n",
    "\n",
    "    # ---- 10. decoded strings (use raw tokens with Ġ) ----\n",
    "    def safe_decode(token_list):\n",
    "        if not token_list:\n",
    "            return \"\"\n",
    "        ids = tokenizer.convert_tokens_to_ids(token_list)\n",
    "        if None in ids:\n",
    "            return \"\"\n",
    "        return tokenizer.decode(ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    decoded_inputs  = [safe_decode(raw) for raw in all_input_raw]\n",
    "    decoded_outputs = [safe_decode(raw) for raw in all_output_raw]\n",
    "\n",
    "    # ---- 11. return everything ----\n",
    "    return (\n",
    "        \"\", history,\n",
    "        f\"{all_input_counts}\",\n",
    "        f\"{all_output_counts}\",\n",
    "        f\"{all_input_strings}\",\n",
    "        f\"{decoded_inputs}\",\n",
    "        f\"{all_output_strings}\",\n",
    "        f\"{decoded_outputs}\"\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Clear\n",
    "# -------------------------------------------------\n",
    "def clear_chat():\n",
    "    global conversation, all_input_counts, all_output_counts\n",
    "    global all_input_strings, all_output_strings, all_input_raw, all_output_raw\n",
    "    conversation = []\n",
    "    all_input_counts = all_output_counts = []\n",
    "    all_input_strings = all_output_strings = []\n",
    "    all_input_raw = all_output_raw = []\n",
    "    return [], \"\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\", \"[]\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. UI\n",
    "# -------------------------------------------------\n",
    "with gr.Blocks(title=\"Llama-3-8b-Clean Tokens\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"# Llama-3.2-1B-Instruct Chat\\n\"\n",
    "        \"**Input Tokens** = system + history + user (content only, no tags).\\n\"\n",
    "        \"`Ġ` is stripped **only** for the token-string display.\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            txt = gr.Textbox(placeholder=\"Type and press Enter…\", label=\"Message\")\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"**Input count (content-only):**\");   i_cnt = gr.Markdown(\"[]\")\n",
    "            gr.Markdown(\"**Output count (raw):**\");          o_cnt = gr.Markdown(\"[]\")\n",
    "            gr.Markdown(\"**Input tokens (no Ġ):**\");         i_str = gr.Markdown(\"[]\")\n",
    "            gr.Markdown(\"**Decoded input:**\");               i_dec = gr.Markdown(\"[]\")\n",
    "            gr.Markdown(\"**Output tokens (no Ġ):**\");        o_str = gr.Markdown(\"[]\")\n",
    "            gr.Markdown(\"**Decoded output:**\");              o_dec = gr.Markdown(\"[]\")\n",
    "\n",
    "    gr.Button(\"Clear\").click(\n",
    "        clear_chat,\n",
    "        None,\n",
    "        [chatbot, txt, i_cnt, o_cnt, i_str, i_dec, o_str, o_dec]\n",
    "    )\n",
    "    txt.submit(\n",
    "        chat,\n",
    "        [txt, chatbot],\n",
    "        [txt, chatbot, i_cnt, o_cnt, i_str, i_dec, o_str, o_dec]\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Launch\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True, share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
